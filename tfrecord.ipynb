{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GniyyK053Yj"
   },
   "source": [
    "## Introduction + Set Up\n",
    "\n",
    "TFRecords store a sequence of binary records, read linearly. They are useful format for\n",
    "storing data because they can be read efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ROIOPAcC53Yk"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib' has no attribute 'distribute'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-a0f87d4c6d15>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mtpu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcluster_resolver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTPUClusterResolver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Device:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtpu\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaster\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow.contrib' has no attribute 'distribute'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-a0f87d4c6d15>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mstrategy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTPUStrategy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtpu\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0mstrategy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Number of replicas:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_replicas_in_sync\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow.contrib' has no attribute 'distribute'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    print(\"Device:\", tpu.master())\n",
    "    strategy = tf.contrib.distribute.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEbyYx4z53Ym"
   },
   "source": [
    "We want a bigger batch size as our data is not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5QwlDIt53Ym"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "GCS_PATH = \"gs://kds-b38ce1b823c3ae623f5691483dbaa0f0363f04b0d6a90b63cf69946e\"\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = [1024, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXO13zYc53Yn"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adC4DBVv53Yn"
   },
   "outputs": [],
   "source": [
    "FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/tfrecords/train*.tfrec\")\n",
    "split_ind = int(0.9 * len(FILENAMES))\n",
    "TRAINING_FILENAMES, VALID_FILENAMES = FILENAMES[:split_ind], FILENAMES[split_ind:]\n",
    "\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/tfrecords/test*.tfrec\")\n",
    "print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\n",
    "print(\"Validation TFRecord Files:\", len(VALID_FILENAMES))\n",
    "print(\"Test TFRecord Files:\", len(TEST_FILENAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ0CS8Ze53Yo"
   },
   "source": [
    "### Decoding the data\n",
    "\n",
    "The images have to be converted to tensors so that it will be a valid input in our model.\n",
    "As images utilize an RBG scale, we specify 3 channels.\n",
    "\n",
    "We also reshape our data so that all of the images will be the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vesRJMmR53Yp"
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oudvqdGq53Yq"
   },
   "source": [
    "As we load in our data, we need both our `X` and our `Y`. The X is our image; the model\n",
    "will find features and patterns in our image dataset. We want to predict Y, the\n",
    "probability that the lesion in the image is malignant. We will to through our TFRecords\n",
    "and parse out the image and the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2sWzszk53Yq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = ({\"image\":tf.io.FixedLenFeature([], tf.string), \"target\":tf.io.FixedLenFeature([], tf.int64), } if labeled else {\"image\":tf.io.FixedLenFeature([], tf.string), })\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example[\"image\"])\n",
    "    if labeled:\n",
    "        label = tf.cast(example[\"target\"], tf.int32)\n",
    "        return image, label\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwEndVMG53Yr"
   },
   "source": [
    "### Define loading methods\n",
    "\n",
    "Our dataset is not ordered in any meaningful way, so the order can be ignored when\n",
    "loading our dataset. By ignoring the order and reading files as soon as they come in, it\n",
    "will take a shorter time to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOVmgS7h53Yr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(filenames, labeled=True):\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(filenames)  # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order)  # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nkMdAgA53Ys"
   },
   "source": [
    "We define the following function to get our different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU9TQuWr53Ys"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(filenames, labeled=True):\n",
    "    dataset = load_dataset(filenames, labeled=labeled)\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YhdQGRs53Yt"
   },
   "source": [
    "### Visualize input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMr683M453Yt"
   },
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(TRAINING_FILENAMES)\n",
    "valid_dataset = get_dataset(VALID_FILENAMES)\n",
    "test_dataset = get_dataset(TEST_FILENAMES, labeled=False)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5, 5, n+1)\n",
    "        plt.imshow(image_batch[n]/255.0)\n",
    "        if label_batch[n]:\n",
    "            plt.title(\"MALIGNANT\")\n",
    "        else:\n",
    "            plt.title(\"BENIGN\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAILWpL553Yu"
   },
   "source": [
    "## Building our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN7JTSqK53Yu"
   },
   "source": [
    "### Define callbacks\n",
    "\n",
    "The following function allows for the model to change the learning rate as it runs each\n",
    "epoch.\n",
    "\n",
    "We can use callbacks to stop training when there are no improvements in the model. At the\n",
    "end of the training process, the model will restore the weights of its best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxewbKg353Yu"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"melanoma_model.h5\", save_best_only=True)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSShHGSX53Yv"
   },
   "source": [
    "### Build our base model\n",
    "\n",
    "Transfer learning is a great way to reap the benefits of a well-trained model without\n",
    "having the train the model ourselves. For this notebook, we want to import the Xception\n",
    "model. A more in-depth analysis of transfer learning can be found\n",
    "[here](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/).\n",
    "\n",
    "We do not want our metric to be ```accuracy``` because our data is imbalanced. For our\n",
    "example, we will be looking at the area under a ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdr7fCj953Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_model():\n",
    "    base_model = tf.keras.applications.Xception(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\")\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input([*IMAGE_SIZE, 3])\n",
    "    x = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    x = base_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.7)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss=\"binary_crossentropy\", metrics=tf.keras.metrics.AUC(name=\"auc\"), )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWrJFtOG53Yv"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbFsV_qR53Yw"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = make_model()\n",
    "\n",
    "history = model.fit(train_dataset, epochs=2, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPz-RYUM53Yw"
   },
   "source": [
    "## Predict results\n",
    "\n",
    "We'll use our model to predict results for our test dataset images. Values closer to `0`\n",
    "are more likely to be benign and values closer to `1` are more likely to be malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg90JFu853Yw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_batch_predictions(image_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5, 5, n+1)\n",
    "        plt.imshow(image_batch[n]/255.0)\n",
    "        img_array = tf.expand_dims(image_batch[n], axis=0)\n",
    "        plt.title(model.predict(img_array)[0])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "image_batch = next(iter(test_dataset))\n",
    "\n",
    "show_batch_predictions(image_batch)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tfrecord",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}